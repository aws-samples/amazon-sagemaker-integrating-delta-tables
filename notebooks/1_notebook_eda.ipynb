{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce535da8-cb8b-4d7b-a8f1-740d2a229f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# From Delta Lake to Amazon SageMaker\n",
    "\n",
    "[Delta Lake](https://delta.io/) is a common open-source framework used for storing data in Lakehouse architectures.\n",
    "\n",
    "In this sample we demonstrate how to integrate Delta Tables with Amazon SageMaker for performing data exploration, ingestion, processing, training, and hosting for Machine Learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 1 - Data Exploration and Visualization\n",
    "\n",
    "***Use Kernel \"Data Science 3.0 (Python 3)\" for running this notebook***\n",
    "\n",
    "In this notebook, we will perform some Exploratory Data Analysis (EDA) over our Delta Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708acb00-a01e-4f68-a2e2-be6b3a30aa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.130.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51406204-684a-475c-9648-a4c189e4e790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73e2b62-5677-41c5-ae84-ba43fac6269d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default bucket: sagemaker-eu-west-1-889960878219\n"
     ]
    }
   ],
   "source": [
    "# S3 bucket for saving processing job outputs\n",
    "sm_session = sagemaker.Session()\n",
    "bucket = sm_session.default_bucket()\n",
    "region = sm_session.boto_region_name\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "print('Default bucket: '+bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af9c9f6-abcd-4cb9-93ff-9ebc8ab629e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db58041-0426-41ad-ae0d-6fa4565889a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages: io.delta:delta-core_2.12:1.1.0,org.apache.hadoop:hadoop-aws:3.2.2\n"
     ]
    }
   ],
   "source": [
    "# Build list of packages entries using Maven coordinates (groupId:artifactId:version)\n",
    "pkg_list = []\n",
    "pkg_list.append(\"io.delta:delta-core_2.12:1.1.0\")\n",
    "pkg_list.append(\"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "\n",
    "packages=(\",\".join(pkg_list))\n",
    "print('packages: '+packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6a36a2-e157-4ca6-b5d8-9800ba6ce0fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c2e80bfc-2ebd-456a-a206-b0984bf78362;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 660ms :: artifacts dl 35ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c2e80bfc-2ebd-456a-a206-b0984bf78362\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/17ms)\n",
      "23/01/30 10:56:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/30 10:56:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Spark via builder\n",
    "# Note: we use the `ContainerCredentialsProvider` to give us access to underlying IAM role permissions\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"PySparkApp\") \n",
    "    .config(\"spark.jars.packages\", packages) \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "    .config(\"fs.s3a.aws.credentials.provider\",'com.amazonaws.auth.ContainerCredentialsProvider') \n",
    "    .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print('Spark version: '+str(sc.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130b6898-288c-4501-b9aa-8188b3c593d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/delta_format/\n"
     ]
    }
   ],
   "source": [
    "s3a_delta_table_uri=f's3a://{bucket}/delta_to_sagemaker/delta_format/'\n",
    "print(s3a_delta_table_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "579397e4-f051-4e27-ba75-f99cc3feeeaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL command: SELECT * FROM delta.`s3a://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/delta_format/` ORDER BY timestamp\n"
     ]
    }
   ],
   "source": [
    "# Create SQL command inserting the S3 path location\n",
    "\n",
    "sql_cmd = f'SELECT * FROM delta.`{s3a_delta_table_uri}` ORDER BY timestamp'\n",
    "print(f'SQL command: {sql_cmd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36012f7-91b1-4911-8237-b97f1a2e0dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/30 10:56:55 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+------+-------+--------------+-----------+--------------+\n",
      "|rowID| timestamp|ratingID|userID|placeID|rating_overall|rating_food|rating_service|\n",
      "+-----+----------+--------+------+-------+--------------+-----------+--------------+\n",
      "|    0|2022-08-25|    3416|    gK|    681|             1|          2|             2|\n",
      "|    1|2022-08-25|    3417|    gK|    719|             1|          1|             1|\n",
      "|    2|2022-08-25|    3418|    gK|   1128|             1|          2|             2|\n",
      "|    3|2022-08-25|    3419|    gK|   1203|             1|          2|             2|\n",
      "|    4|2022-08-25|    3420|    gK|   1058|             1|          1|             1|\n",
      "|    5|2022-08-25|    3421|    gK|    585|             1|          0|             0|\n",
      "|    6|2022-08-25|    3422|    gL|    990|             2|          2|             2|\n",
      "|    7|2022-08-25|    3423|    gL|   1192|             2|          2|             2|\n",
      "|    8|2022-08-25|    3424|    gL|   1390|             2|          2|             2|\n",
      "|    9|2022-08-25|    3425|    gL|   1052|             2|          2|             2|\n",
      "+-----+----------+--------+------+-------+--------------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Execute SQL command which returns dataframe\n",
    "sql_results = spark.sql(sql_cmd)\n",
    "print(type(sql_results))\n",
    "\n",
    "sql_results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323048a7-c229-4d3d-8ece-d53401a30412",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sagemaker_datawrangler\n",
    "\n",
    "df = sql_results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91c12ea7-6d6c-40c7-9ef5-a3bd6b08b4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fed635fe2b4c6fa1dbb36ee78a740f"
      },
      "text/plain": [
       "     rowID   timestamp ratingID userID placeID rating_overall rating_food  \\\n",
       "0        0  2022-08-25     3416     gK     681              1           2   \n",
       "1        1  2022-08-25     3417     gK     719              1           1   \n",
       "2        2  2022-08-25     3418     gK    1128              1           2   \n",
       "3        3  2022-08-25     3419     gK    1203              1           2   \n",
       "4        4  2022-08-25     3420     gK    1058              1           1   \n",
       "...    ...         ...      ...    ...     ...            ...         ...   \n",
       "8443  8443  2022-08-25    11859     zV     984              1           1   \n",
       "8444  8444  2022-08-25    11860     zV    1311              0           1   \n",
       "8445  8445  2022-08-25    11861     zV    1025              1           2   \n",
       "8446  8446  2022-08-25    11862     zV     871              1           2   \n",
       "8447  8447  2022-08-25    11863     zV     432              1           1   \n",
       "\n",
       "     rating_service  \n",
       "0                 2  \n",
       "1                 1  \n",
       "2                 2  \n",
       "3                 2  \n",
       "4                 1  \n",
       "...             ...  \n",
       "8443              2  \n",
       "8444              2  \n",
       "8445              2  \n",
       "8446              2  \n",
       "8447              2  \n",
       "\n",
       "[8448 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e188b0-3753-4dcc-8868-0fd8b3b920ae",
   "metadata": {},
   "source": [
    "Running the cell above should open the interactive data preparation widget embedded in your notebook, powered by SageMaker Data Wrangler.\n",
    "\n",
    "This will allow you getting insights on the data, as well as recommendations for suggested transforms for improving the quality of the data in preparation for training.\n",
    "\n",
    "<center><img src=\"../images/DeltaLake_to_SageMaker_1.png\" width=\"60%\"></center>\n",
    "\n",
    "----\n",
    "\n",
    "In the following notebook, we will rely on SageMaker Processing for performing these transformations, but note you could alternatively also run these with SageMaker Data Wrangler just by clicking on the \"Apply and export code\" buttons in the suggested transforms directly."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
