{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e022ec1-83e7-462c-bf8b-8c85b0374292",
   "metadata": {
    "tags": []
   },
   "source": [
    "# From Delta Lake to Amazon SageMaker\n",
    "\n",
    "[Delta Lake](https://delta.io/) is a common open-source framework used for storing data in Lakehouse architectures.\n",
    "\n",
    "In this sample we demonstrate how to integrate Delta Tables with Amazon SageMaker for performing data exploration, ingestion, processing, training, and hosting for Machine Learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 0 - Connection Set-up - Via Local Spark Session and Delta Sharing\n",
    "\n",
    "***Use Kernel \"Data Science 3.0 (Python 3)\" for running this notebook***\n",
    "\n",
    "In this notebook, we will setup a connection between Amazon SageMaker and a Delta Table. We will do this with two different methods:\n",
    "1. By establishing a connection with a Delta Table in Amazon S3 via [Spark Sessions](https://sparkbyexamples.com/pyspark/pyspark-what-is-sparksession/)\n",
    "2. By establishing a connection with a Delta Table via [Delta Sharing](https://delta.io/sharing/)\n",
    "\n",
    "<center><img src=\"../images/DeltaLake_to_SageMaker_0.png\" width=\"50%\"></center>\n",
    "\n",
    "\n",
    "Each methods has its own pros and cons, like e.g.:\n",
    "\n",
    "* Connection via Spark Session allows you to interactively query the data in the Delta Table, without the need to materialize a full copy of the table in memory.\n",
    "* Connection via Spark Session allows you to rely on AWS IAM roles for managing access and control to the data.\n",
    "* Connection via Delta Sharing allows you to rely on tokens centrally managed by the Delta Lake's Delta Sharing Server, for read-only access via short-lived URLs.\n",
    "* Connection via Delta Sharing simplifies the access since it just require a single library (delta_sharing) installed in the client.\n",
    "* Connection via Delta Sharing allows you to read the tables as either Pandas or Spark data-frames.\n",
    "\n",
    "In the following cells we will explore both alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90710f1-3859-4f19-9869-56228b2df0c4",
   "metadata": {},
   "source": [
    "Let's start by making sure we have an updated version of the SageMaker SDK, and install the other libraries required for this sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007f26e2-8ac1-4944-827f-92308cf7e585",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.120.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.130.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.23.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Collecting boto3<2.0,>=1.26.28\n",
      "  Downloading boto3-1.26.59-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.4.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Collecting botocore<1.30.0,>=1.29.59\n",
      "  Downloading botocore-1.29.59-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.59->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\n",
      "Installing collected packages: botocore, boto3, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.24\n",
      "    Uninstalling botocore-1.29.24:\n",
      "      Successfully uninstalled botocore-1.29.24\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.24\n",
      "    Uninstalling boto3-1.26.24:\n",
      "      Successfully uninstalled boto3-1.26.24\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.120.0\n",
      "    Uninstalling sagemaker-2.120.0:\n",
      "      Successfully uninstalled sagemaker-2.120.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.24 requires botocore==1.29.24, but you have botocore 1.29.59 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.59 botocore-1.29.59 sagemaker-2.130.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf389a1-afdb-4750-98f8-d2d1276ff8e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.01.10 |       h06a4308_0         120 KB\n",
      "    certifi-2022.12.7          |  py310h06a4308_0         150 KB\n",
      "    conda-23.1.0               |  py310h06a4308_0         953 KB\n",
      "    openjdk-11.0.13            |       h87a67e3_0       341.0 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       342.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            pkgs/main/linux-64::openjdk-11.0.13-h87a67e3_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2022.9.2~ --> pkgs/main::ca-certificates-2023.01.10-h06a4308_0 \n",
      "  certifi            conda-forge/noarch::certifi-2022.9.24~ --> pkgs/main/linux-64::certifi-2022.12.7-py310h06a4308_0 \n",
      "  conda                             22.11.0-py310h06a4308_1 --> 23.1.0-py310h06a4308_0 \n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install openjdk -q -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e4fc75-88da-409c-8537-2570acbbbd84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.2.0\n",
      "  Using cached pyspark-3.2.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.2\n",
      "  Using cached py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349cbc14-ed0d-4342-936d-2bb1eeedce88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark==1.1.0\n",
      "  Using cached delta_spark-1.1.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from delta-spark==1.1.0) (4.11.3)\n",
      "Requirement already satisfied: pyspark<3.3.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from delta-spark==1.1.0) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=1.0.0->delta-spark==1.1.0) (3.8.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /opt/conda/lib/python3.10/site-packages (from pyspark<3.3.0,>=3.2.0->delta-spark==1.1.0) (0.10.9.2)\n",
      "Installing collected packages: delta-spark\n",
      "Successfully installed delta-spark-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install delta-spark==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c09f92-1c61-4230-bdca-7089f0ecdfc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-sharing\n",
      "  Using cached delta_sharing-0.6.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from delta-sharing) (10.0.1)\n",
      "Requirement already satisfied: fsspec>=0.7.4 in /opt/conda/lib/python3.10/site-packages (from delta-sharing) (2022.7.1)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from delta-sharing) (2.28.1)\n",
      "Collecting yarl>=1.6.0\n",
      "  Using cached yarl-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from delta-sharing) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow>=4.0.0->delta-sharing) (1.23.5)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl>=1.6.0->delta-sharing) (3.3)\n",
      "Collecting multidict>=4.0\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->delta-sharing) (2.0.4)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->delta-sharing) (21.4.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->delta-sharing) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->delta-sharing) (2022.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->delta-sharing) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->delta-sharing) (1.26.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->delta-sharing) (1.16.0)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, delta-sharing\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 delta-sharing-0.6.2 frozenlist-1.3.3 multidict-6.0.4 yarl-1.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install delta-sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4844f2-a3a6-4c1e-bab1-38f62a2eb0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.130.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0449e02-4114-4c19-ad31-2ab66488cb84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61395998-70f2-4360-adb9-38701c55849d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default bucket: sagemaker-eu-west-1-889960878219\n"
     ]
    }
   ],
   "source": [
    "# S3 bucket for saving processing job outputs\n",
    "sm_session = sagemaker.Session()\n",
    "bucket = sm_session.default_bucket()\n",
    "region = sm_session.boto_region_name\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "print('Default bucket: '+bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fcb8d0-4398-4669-b9f9-5dcc8330d245",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Option 1: Connecting to a Delta Table in Amazon S3 via Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8262f6-0dc4-444a-ab82-d7fb5a58aa24",
   "metadata": {},
   "source": [
    "In this section we will establish a Spark Session for interacting with a Delta Table stored in Amazon S3. For this purpose, we will:\n",
    "* Install some libraries required\n",
    "* Establish a Spark Session and Context\n",
    "* Upload a sample dataset to Amazon S3, and write it as a Delta Table\n",
    "* Test our connection towards the Delta Table and verify the format\n",
    "\n",
    "We can now import the required libraries, and setup the packages required for our Spark Session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e62081-f0d5-46d7-93ab-520949a9b4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3747182b-0676-4056-b003-24975a239eae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages: io.delta:delta-core_2.12:1.1.0,org.apache.hadoop:hadoop-aws:3.2.2\n"
     ]
    }
   ],
   "source": [
    "# Build list of packages entries using Maven coordinates (groupId:artifactId:version)\n",
    "pkg_list = []\n",
    "pkg_list.append(\"io.delta:delta-core_2.12:1.1.0\")\n",
    "pkg_list.append(\"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "\n",
    "packages=(\",\".join(pkg_list))\n",
    "print('packages: '+packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7d8b5-a73b-4e36-b2c3-6ef4822d7f41",
   "metadata": {},
   "source": [
    "We can now establish the Spark Session and opening a Spark Context..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07721cbf-eaab-4bad-951d-db3d194fbc69",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d850dfd4-b2b0-494c-a259-abfea31aab67;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 994ms :: artifacts dl 69ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d850dfd4-b2b0-494c-a259-abfea31aab67\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/55ms)\n",
      "23/01/30 10:47:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Spark via builder\n",
    "# Note: we use the `ContainerCredentialsProvider` to give us access to underlying IAM role permissions\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"PySparkApp\") \n",
    "    .config(\"spark.jars.packages\", packages) \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "    .config(\"fs.s3a.aws.credentials.provider\",'com.amazonaws.auth.ContainerCredentialsProvider') \n",
    "    .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print('Spark version: '+str(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0cf1e2-0981-4259-8793-9ab6880c4a73",
   "metadata": {},
   "source": [
    "For this example, we will create a table in Amazon S3 by uploading a sample synthetic dataset for fact rating, and writting it as a Delta Table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fdd386f-bf95-452b-86f3-b1052bb4b255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/raw_csv/fact_rating_synthetic.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "local_basename = 'fact_rating_synthetic.csv'\n",
    "local_file = '../data/' + local_basename\n",
    "upload_s3_uri = f's3://{bucket}/delta_to_sagemaker/raw_csv'\n",
    "\n",
    "S3Uploader.upload(local_file, upload_s3_uri, sagemaker_session=sm_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee90e5ec-3e19-4d36-bd6c-6069cd6af219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/raw_csv/fact_rating_synthetic.csv\n"
     ]
    }
   ],
   "source": [
    "# Load raw data from S3 location\n",
    "\n",
    "s3_raw_csv = f's3://{bucket}/delta_to_sagemaker/raw_csv/fact_rating_synthetic.csv'\n",
    "s3a_raw_csv = s3_raw_csv.replace('s3:','s3a:')\n",
    "\n",
    "print(s3a_raw_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "246f3f4e-695f-430b-a906-0d37d36e8b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/30 10:48:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.27 ms, sys: 3.43 ms, total: 11.7 ms\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rating_df = spark.read.csv(s3a_raw_csv, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6281146-92d2-4773-8608-5bfe953feb35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 8448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rowID', 'string'),\n",
       " ('timestamp', 'string'),\n",
       " ('ratingID', 'string'),\n",
       " ('userID', 'string'),\n",
       " ('placeID', 'string'),\n",
       " ('rating_overall', 'string'),\n",
       " ('rating_food', 'string'),\n",
       " ('rating_service', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Rows: '+str(rating_df.count()))\n",
    "rating_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0019150-16fa-42f8-b0d0-cddc37ef46ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+------+-------+--------------+-----------+--------------+\n",
      "|rowID| timestamp|ratingID|userID|placeID|rating_overall|rating_food|rating_service|\n",
      "+-----+----------+--------+------+-------+--------------+-----------+--------------+\n",
      "|    0|2022-08-25|    3416|    gK|    681|             1|          2|             2|\n",
      "|    1|2022-08-25|    3417|    gK|    719|             1|          1|             1|\n",
      "|    2|2022-08-25|    3418|    gK|   1128|             1|          2|             2|\n",
      "|    3|2022-08-25|    3419|    gK|   1203|             1|          2|             2|\n",
      "|    4|2022-08-25|    3420|    gK|   1058|             1|          1|             1|\n",
      "|    5|2022-08-25|    3421|    gK|    585|             1|          0|             0|\n",
      "|    6|2022-08-25|    3422|    gL|    990|             2|          2|             2|\n",
      "|    7|2022-08-25|    3423|    gL|   1192|             2|          2|             2|\n",
      "|    8|2022-08-25|    3424|    gL|   1390|             2|          2|             2|\n",
      "|    9|2022-08-25|    3425|    gL|   1052|             2|          2|             2|\n",
      "+-----+----------+--------+------+-------+--------------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ae33f1-7a5a-4e15-846a-cba1f0a4f930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/delta_format/\n"
     ]
    }
   ],
   "source": [
    "# Write dataframe to Delta Table location using 's3a' protocol\n",
    "s3_delta_table_uri=f's3://{bucket}/delta_to_sagemaker/delta_format/'\n",
    "s3a_delta_table_uri=s3_delta_table_uri.replace('s3:','s3a:')\n",
    "\n",
    "print(s3a_delta_table_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d232e21b-ca2e-40cd-94b6-6c92db85e691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rating_df.write.format(\"delta\").mode(\"overwrite\").save(s3a_delta_table_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1856f5d8-ce1c-49b2-acc4-bd1cce7476dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this a Delta Table?:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "# Use static method to determine table type\n",
    "print(f'Is this a Delta Table?:\\n{DeltaTable.isDeltaTable(spark, s3a_delta_table_uri)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb26379-5980-400e-ab1a-9f1f76aa3513",
   "metadata": {},
   "source": [
    "We now have our sample Delta Table prepared and connected via the Spark Session.\n",
    "\n",
    "Note, you can replace the URI of the Delta Table above for connecting to your own tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebaeffb-15bc-4405-8e6b-2df8056215db",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "### Option 2: Connecting to a Delta Table via [Delta Sharing](https://delta.io/sharing/)\n",
    "\n",
    "In this section we will connect directly towards an external Delta Table by using the open-source library Delta Sharing. For being able to use this method you should have a Delta Sharing Server available in your Delta Lake for managing accesses and permissions, you can check further details in the blog post [here](https://aws.amazon.com/blogs/opensource/delta-sharing-on-aws/).\n",
    "\n",
    "Note this code is using a `profile_file` that contains the endpoint of the Delta Sharing server hosted either at Databricks or an open-source implementation, together with a bearer token that allows you to access the data.\n",
    "Typically this file is managed and secured on the client-side. Because our experiment with Delta Sharing is about reading data from the Databricks server, we can stick with the provided example profile_file on GitHub and retrieve it via HTTP.\n",
    "\n",
    "For this purpose we will:\n",
    "* Configure the profile for Delta Sharing\n",
    "* Establish a Delta Sharing Client\n",
    "* Load data from our Delta Table and read it as a Pandas data-frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7a462-865e-48cb-858e-da6c6a090624",
   "metadata": {},
   "source": [
    "This time, we will read the [Boston Hosing](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) dataset directly from the Delta-IO [Delta Sharing repository](https://github.com/delta-io/delta-sharing)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68aae6c0-04b6-4f4d-8ac5-1cd0aa2024be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-30 10:53:43--  https://raw.githubusercontent.com/delta-io/delta-sharing/main/examples/open-datasets.share\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 148 [text/plain]\n",
      "Saving to: ‘open-datasets.share’\n",
      "\n",
      "en-datasets.share   100%[===================>]     148  --.-KB/s    in 0s      \n",
      "\n",
      "2023-01-30 10:53:44 (9.87 MB/s) - ‘open-datasets.share’ saved [148/148]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_file = \"https://raw.githubusercontent.com/delta-io/delta-sharing/main/examples/open-datasets.share\"\n",
    "!wget {profile_file} -P ./ -O 'open-datasets.share'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "469ea953-7e8d-4de0-92df-a01a122401cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shareCredentialsVersion\": 1,\n",
      "  \"endpoint\": \"https://sharing.delta.io/delta-sharing/\",\n",
      "  \"bearerToken\": \"faaie590d541265bcab1f2de9813274bf233\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ./open-datasets.share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b88ff41-246e-417d-8713-2e8cda94f6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-889960878219/delta_to_sagemaker/delta_sharing/profile/open-datasets.share\n"
     ]
    }
   ],
   "source": [
    "sample_profile_file_url = sagemaker.Session().upload_data(\n",
    "    './open-datasets.share', bucket=bucket, key_prefix='delta_to_sagemaker/delta_sharing/profile'\n",
    ")\n",
    "\n",
    "print(sample_profile_file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1954b94e-9228-4d3c-829a-c5bc3b293576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a SharingClient\n",
    "import delta_sharing\n",
    "\n",
    "client = delta_sharing.SharingClient(sample_profile_file_url)\n",
    "table_url = profile_file + '#delta_sharing.default.boston-housing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "795c51ce-a29e-4a50-940d-ddf530eb9953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading boston-housing table from Delta Lake\n",
      "Train data shape: (506, 15)\n"
     ]
    }
   ],
   "source": [
    "# Load the table as a Pandas DataFrame\n",
    "print('Loading boston-housing table from Delta Lake')\n",
    "train_data = delta_sharing.load_as_pandas(table_url)\n",
    "print(f'Train data shape: {train_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b48c56f6-cc5e-4afa-89b9-0fffe015222c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5</td>\n",
       "      <td>311</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "0   1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1   2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2   4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
       "3   5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
       "4   7  0.08829  12.5   7.87     0  0.524  6.012  66.6  5.5605    5  311   \n",
       "\n",
       "   ptratio   black  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     18.7  394.63   2.94  33.4  \n",
       "3     18.7  396.90   5.33  36.2  \n",
       "4     15.2  395.60  12.43  22.9  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7da9a-399e-4891-9c08-dcd030b2920b",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "In the following notebook, we will explore a 3rd method for connecting to our Delta Lake table by relying on AWS Glue Interactive Sessions in SageMaker Studio."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
